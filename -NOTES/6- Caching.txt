
In this section, we will talk about caching which is a performance optimization technique.



- What is Caching
  
  If you have a complex query that takes a while to execute, you can use caching to boost our application's performance.
  So the first time the query is executed on the database, then we get the result, and store it in the memory.
  
  Getting the data from the memory is often, but not always, faster than getting it from a database.
  So we'll serve subsequent requests using the data in the cache. So the future requests will have a shorter response time 
  and our web and database servers will be freed up to process more requests.
  
  On the surface, that sounds great; but there is a problem with caching.
  The problem is that if the data in our database gets updated, our cache is not going to get updated. So the data is going to 
  be stale or out-of-date until the cache expires. Typically when we store an object in the cache, we can put an expiration time 
  on it, like five minutes or three hours, depending on how frequently the data gets updated. So if the data gets updated 
  frequently and we should always show the most up-to-date data to the user, then there is really no point storing it in the 
  cache. 
  
  Caching is not limited to the result of database queries. For example, you might have to call a third party API to get some 
  data. If that API is slow or even becomes unavailable, we can improve our applications performance by storing the result 
  in the cache. We can also run a celery job to update the cache in the background every now and then.
  
  So, overall, caching is a great optimization technique but as we always say: Too much of a good thing is a bad thing.
  So if you cache data aggressively, you will need a lot of memory, and that's going to cost you a fair bit.
  But also caching may even reduce the performance of your application; so don't assume that just because reading data from 
  the memory is faster than reading it from the disk, you should always store your data in the cache.
  Some data queries get executed pretty quickly, and your application will respond faster if you simply get the data from the 
  database every time. The only way to know this is by running a performance test before and after optimization.
  So don't make assumptions without a proper performance test. Again this old saying: 
  Premature optimization is the root of all evil.
  

- Cache Backends
  
  Django comes with various types of cache backends, or cache engines:
        
        Local memory (default), which stores data in the same process that runs are Django application. 
    
    Local memory is good for development but not a good choice for production. In a production environment, we should use 
    a proper caching server. And for that:
        
        Memcached 
        
        Redis
        
    Both of these are enterprise cache server; so you can choose either. 
    In this course, because we have been using Redis as our message broker, I prefer to use it as our cache backend as well;
    so we don't have too many dependencies.

        Database
    
    We can also use our database as a cache; so if you have a complex query that hits multiple tables and takes a while to 
    execute, then we can store the result in the database and quickly pull it out for subsequent requests. This is faster 
    than executing that complex query every time; but it's not as fast as pulling out data from the memory. 
    
        File system
    
    We can also use the file system as a cache; so the result is stored in files. This is not something I've used that often 
    but it's there in Django in case you need it.
    
  So that's all about cache backends. All these backends, except Redis, come with django by default.



- Simulating a Slow API

  For this section, we are going to simulate a slow API endpoint.
  
  Let's go to the say_hello view of the playground app, and make a couple changes:


        from django.shortcuts import render

        # we import requests module built into python, and using this module we can send an HTTP request to another service: 
        import requests


        def say_hello(request):
            # we send a request:
            response = requests.get('https://httpbin.org/delay/2')
            # When we hit this endpoint, the server is going to wait two seconds to respond to us. So the simulates a slow 
            # third-party service. 
            
            return render(request, 'hello.html', {'name': 'Mosh'})
        
        
  Now let's test this. So back in the browser, let's go to => http://127.0.0.1:8000/playground/hello/
  As you can see, we're waiting for two seconds, and then we got the response.


- Getting a Baseline Performance Benchmark

  Now we are going to run a performance test to collect our baseline.
  So let's go to the locustfiles folder, and open our test script (browse_products.py). We're gonna define a new task for 
  hitting our slow api endpoint:
        
        @task
        def say_hello(self):
            self.client.get('/playground/hello/')
            
  Now in the terminal, let's restart locust (500 users, 10 spawn rate).
  Let's make the test continue until we send about 7,000 requests to the server.
  After stopping the test, look at the first row or the hello Endpoint. 90% of our users better response in 3900 milliseconds,
  or less; and our total aggregated metric is 3000. So let's keep this report somewhere; so after we implement caching, 
  we can compare the metrics.


- Installing Redis
  (I have already installed it in previous section.)

- Configuring Caching
  
        pipenv install django-redis
  
  Then go to => https://github.com/jazzband/django-redis    in order to grab a piece of code from there.
  That is our cache configuration. In settings.py:

        CACHES = {
            "default": {
                "BACKEND": "django_redis.cache.RedisCache",
                "LOCATION": "redis://127.0.0.1:6379/1",
                "OPTIONS": {
                    "CLIENT_CLASS": "django_redis.client.DefaultClient",
                }
            }
        }
        
        
  So we are defining our default cache. As you can see, the backend is that is "django_redis.cache.RedisCache".
  The location is the location of the redis server ("redis://127.0.0.1:6379/1").
  Here we're using database 1; but if we assigned this database 1 earlier for another thing (like message broker for celery).
  We can change this number. (I didn't use database 1 in this windows project, but changed to to 2 anyway.):
        
        "LOCATION": "redis://127.0.0.1:6379/2",

  So our configuration is ready. Next, we will learn how to use the cache api.
  

- Using the Low-level Cache API

  Let's talk about using the low-level cache API.
  So back to our playground view:

        # we import the cache object:
        from django.core.cache import cache

  This object has an API for accessing the cache. So it has methods for getting objects from the cache or storing them in the cache.
  
  So, in this view, instead of immediately calling httpbin, we're going to check our cache and see if we have the data we are looking for. 
  If we have the data, then we're going to serve this request using data from the cache. Otherwise, we're going to call httpbin to get 
  the data, and then we'll store the data in the cache for subsequent requests:

    def say_hello(request):
        # Here we also need to pass a key for accessing the data we are looking for. We can call that key anything. We said 'httpbin_result':
        key = 'httpbin_result'
        if cache.get(key) is None:
            response = requests.get('https://httpbin.org/delay/2')
            data = response.json()
            cache.set(key, data)
        return render(request, 'hello.html', {'name': cache.get(key)})

    # So if we don't have the data in the cache, we're going to get it, and store it. Otherwise, we're going to serve this request 
    # using the data from the cache. 
    
  
  Now back to the browser, the first time we hit this endpoint (http://127.0.0.1:8000/playground/hello/), our request is going to be 
  slow; it is going to take two seconds; but all subsequent requests will be served from the cache; so they will be super fast.
  
  Our implementation works.
  
  How long is the data going to store in the cache?
  The default timeout is 5 minutes (300 seconds) but we can easily change this. 
  So when storing the data in the cache, here we can supply a timeout:
        
        cache.set(key, data, 10 * 60)
        # '10 * 60' means 10 minutes.
  
  Alternatively, instead of repeating this everywhere, we can set the timeout globally in cache configuration setting. In settings.py:
  
        CACHES = {
            "default": {
                .
                .
                'TIMEOUT': 10 * 60,
                .
                . 
                
  So, using the low-level cache API, by calling the get and set method, we have precise control over caching objects.
  But using this API in every view that is caching is going to be a little bit tedious; it's gonna be repetitive.
  Every time we have to define a key, then repeat logic like we wrote. 
  So in the next lesson we are going to learn a simpler way to cache data.



  - Caching Views
    
    Let's learn a better way to cache our views.
    
    Instead of repeating the logic we wrote in the say_hello function in the last lesson; we can use a decorator in every view 
    that requires caching. So in storefront.views.py:
    
            from django.views.decorators.cache import cache_page
            
            # then we apply it to our view, and give it a timeout:
            @cache_page(5 * 60)
            def say_hello(request):
    
    With this change, we can get rid of the entire caching logic in the function, and simplify it:
    
            
            @cache_page()
            def say_hello(request):
                response = requests.get('https://httpbin.org/delay/2')
                data = response.json()
                return render(request, 'hello.html', {'name': data})
    
    With this implementation, it looks like we have no caching; but the entire caching is controlled by the decorator.
    So let's test it in the browser => http://127.0.0.1:8000/playground/hello/
    Our implementation works.

    When using caching, there is something you need to be aware of.
    Let's go back to our view and make a small change. For example, instead of passing the data, we will pass our name:
    
            return render(request, 'hello.html', {'name': 'Mosh'})
            
     Now, let's refresh the page. As you can see, we are not seeing my name because the data is coming from the cache.
     So we have to wait until the cache expires; in this case, 5 minutes. 
     There are situations where you want to deliberately expire the cache. We will talk about that later in this section. 
     All we need to know here is that if we make any changes, our changes are not going to be visible until the cache is expired.
     
     
     Now, here we used a function-based view; but what if we had a class-based view?
     The cache_page decorator doesn't work on class-based views. For that, we need another decorator. 
     As an example, let's convert this function-based view to a class-based view:
     
            
            from rest_framework.views import APIView
            from django.utils.decorators import method_decorator
            

            class HelloView(APIView):
            @method_decorator(cache_page(5 * 60))
            def get(self, request):
                response = requests.get('https://httpbin.org/delay/2')
                data = response.json()
                return render(request, 'hello.html', {'name': 'Mosh'})
      
      
      Next, we need to register this in our urls modules. So in playground.urls.py:

            urlpatterns = [
                path('hello/', views.HelloView.as_view())
            ]
            
      
      Let's test our implementation.
      Our implementation works.
      
      So, to recap, for function-based views we use cache_page decorator; or class-based views, views method decorator method_decorator.



- Verifying Optimizations
  
  We implemented caching as an optimization technique; now let's run another performance test and see what we get.
  
  First restart locust => locust -f locustfiles/browse_products.py
  Then go to => http://localhost:8089/
  (500 users, 10 spawn rate)
  


- Managing Redis Cache Content
  
  As you know, we are running Redis using Docker. As we have learned, a container is a lightweight virtual machine. 
  Now if we want to go in this virtual machine and execute a command, we want to launch Redis command line interface.
  So we type:

        docker exec -it container_id command_we_want_to_execute
  
  Our command in this case is:

        docker exec -it container_id redis-cli
  
  So we entered the Redis command line interface.
  Here we can send instructions to Redis server. For example; you know that we have two databases; database 1 is used for our message broker;
  database 2 is our cache.
  So, in Redis command line interface, we type:
  
        select 2  
  
  Now, to see all the keys:
  
        keys * 
  
  Currently there is nothing in our cache. 
  So, if you go back to the browser and hit the => http://127.0.0.1:8000/playground/hello/
  now we have two items in the cache. If we want to delete a particular key, we can use the delete command. 
  So we copy the entire key name, and type:
  
        del key_name
  
  If we want to delete all the keys:

        flushall

